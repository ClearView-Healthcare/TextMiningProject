return(text.extract)
}
setwd("~/Documents/MCS/TextMiningClass/Project/TestingFiles")
x<-file.choose()
x
t<-"antonyCleopatra.txt"
format.lda<-extractContentTxt(t)
uris.name=t
text.scan <- scan(uris.name, what="character", sep="\n",blank.lines.skip = TRUE)
texts <- enc2utf8(text.scan)
text.collapse<-paste(texts,collapse=" ")
text.hyphen <- gsub("-\\s+","",text.collapse)
text.space<- gsub("\\s\\s+"," ",text.hyphen)
text.extract[[1]] <-text.space
text.extract <- list()
text.extract[[1]] <-text.space
text.extract <- unlist(text.extract)
text.extract
data<- text.extract[1]
text <- paste(data, collapse = " ")
text
text.punct <- data
if (remove_urls==TRUE) {
text.punct <- rm_email(text.punct)
text.punct <- rm_url(text.punct)
text.punct <- rm_twitter_url(text.punct)
} else {text.punct <- text.punct}
text.punct <- gsub("\\s\\s+"," ",text.punct)
text.punct <- str_c(text.punct)
text.punct<- str_trim(text.punct)
text.punct
text.split<-strsplit(text.punct, " ")
text.split <- unlist(text.split)
text.freq <- table(text.split)
text.relative <- 100*(text.freq/sum(text.freq))
lda.format <- vector()
lda.format[1] <- text.punct
lda.format
test <- function(x){
text.extraction <- vector()
lda.format <- vector()
novel.list <- list()
punct.list <-vector()
novel.lda <- list()
num<-1#length(y$name)
file.names <-"name"#y$name
for (i in 1:num) {
data<- x[i]
# data<- text.extract[1]
text <- paste(data, collapse = " ")
text.punct <- data
text.punct <- gsub("\\s\\s+"," ",text.punct)
text.punct <- str_c(text.punct)
text.punct<- str_trim(text.punct)
text.split<-strsplit(text.punct, " ")
text.split <- unlist(text.split)
data.del <- gsub("[A-Za-z0-9]"," \\1", data)
data.del.w <- paste(data.del, collapse = " ")
data.no.punct<- gsub("([!¿?;,¡:]|(\\.+))", " \\1 ", data.del.w)
data.no.punct <-  gsub("\\s+"," ",data.no.punct)
file.name <- sub("(.*\\/)([^.]+)(\\.[[:alnum:]]+$)", "\\2", file.names[i])#input$file.article.txt$name[i])
# text.split<-strsplit(text.punct, " ")
# text.split <- unlist(text.split)
text.freq <- table(text.split)
text.relative <- 100*(text.freq/sum(text.freq))
novel.list[[file.name]] <- text.relative
punct.list[i] <-data.no.punct
novel.lda[i] <-text.punct
lda.format[i] <- text.punct
text.extraction[i] <- text
}
list(data=data, novel.list=novel.list,text.extraction=text.extraction,punct.list=punct.list,novel.lda=novel.lda,lda.format=lda.format,data=data)
}
lda.format
x<-test(lda.format)
x$data
x$lda.format
runApp('~/R/Tweeter/TextMining')
class(text.extract)
text.extraction
lda.format
corpus.lda <-lda.format
lda.list <- strsplit(corpus.lda, "\\s+")
lda.list
runApp('~/R/Tweeter/TextMining')
class(x)
class(x$lda.format)
runApp('~/R/Tweeter/TextMining')
?scan
library(NLP)
library(openNLP)
library(tm)
library(stringr)
library(openNLPmodels.es)
install.packages("openNLPmodels.es", repos = "http://datacube.wu.ac.at/", type = "source")
library(openNLPmodels.es)
setwd("~/Documents/CL/Classes/Corpus_Linguistics/Course2016/Projects")
twit.file <- read.csv(file.choose())
twit.list <- as.list(twit.file[1])
twit.column1 <-twit.file[1,]
twit.list
twit.column1
length(twit.list)
length(twit.file[1])
twit.list <- as.data.frame(twit.file)
length(twit.list)
head(twit.list)
colnames(twit.list)
twit.data.frame <- as.data.frame(twit.file)
twit.data.frame$text[1]
twit.data.frame$text[2]
twit.data.frame <- as.matrix(twit.file)
twit.data.frame$text  # twits
class(twit.file)
head(twit.file)
twit.csv<-twit.file
class(twit.csv)
twit.csv$text  # twits
twit.table <- as.table(twit.csv)
twits <- twit.csv$text # twits
twits
twits <- as.list(twit.csv$text) # twits
twits
twits <- unlist(as.list(twit.csv$text)) # twits
twits
twit.list <- split(twit.csv$text, seq(nrow(twit.csv$text)))
twit.list <- split(twit.csv$text, nrow(twit.csv$text))
twit.list <- split(twit.csv$text)
?split
twit.list <- split(twit.csv, nrow(twit.csv))
twit.list
class(twit.list)
length(list)
length(twit.list)
twit.list[1]
twit.list[[1]]
twit.list[[1]][1]
twit.list[[1]][2]
twits[1]
twits[1:10]
twit.list[[1]][1]
twits.split <- lapply(strsplit(twits)," ")
?strsplit
twits.split <- lapply(strsplit(twits," "))
as.list(twits)
as.character(twits)
twits<- as.character(twit.list[[1]][1])
twits
twits<- twit.list[[1]][1]
twits <- as.character(twits)
twits
twits<- twit.list[[1]][1]
twits <- as.vector(twits)
twits
names <- as.vector(names)
names <- twit.list[[1]][2]
names <- as.vector(names)
names
twits.split <- lapply(twits, strsplit(twits," "))
twits.split <- lapply(strsplit(as.character(twits)," "))
twits.split <- sapply(strsplit(as.character(twits)," "))
twits.split <- lapply(strsplit(as.character(twits)," "),"[[")
twits.split <- lapply(strsplit(as.character(twits)," "),function(x))
?lapply
twits.split <- lapply(twits, strsplit(as.character(twits)," "))
twits.split <- lapply(twits, function(x) strsplit(as.character(x)," ",fixed=TRUE)[[1]] [1])
twits.split
Sys.setlocale("LC_ALL", "pt_PT.UTF-8")
twits.split <- lapply(twits, function(x) strsplit(as.character(x)," ",fixed=TRUE)[[1]] [1])
twits.split
class(twits)
twitss <- paste(twist, sep=" ")
twitss <- paste(twits, sep=" ")
twitss
twits
twits[1]
twits[[1]]
twits[[1]][1]
test<-twits[[1]][1]
test
twit.csv <- read.csv(file.choose(),stringsAsFactors=FALSE)
twit.csv
twit.csv$text[1:10]
twit.csv$text[1]
strsplit(twit.csv$text[1]," ")
x<-twit.csv$text[1]
encoding(x) <-"UTF-8"
Encoding(x) <-"UTF-8"
x
strsplit(x," ")
x<-twit.csv$text
Encoding(x) <-"UTF-8"
x
my.row <- x
my.row <- x[1]
my.row <- paste("<begin> ",my.row," <end>")
my.row
x.paset <- paste("<begin> ",x," <end>")
x.paset[1]
my.text<-twit.csv$text
Encoding(my.text) <-"UTF-8"
my.text.begin.end<-paste("<begin> ",my.text," <end>")
my.text.begin.end
corpus.tmp <- lapply(my.text.begin.end, function(x){x <- paste(x, collapse = " ")})
corpus.tmp
corpus.tmp <- lapply(corpus.tmp, function(x){x<- enc2utf8(x)})
corpus.tmp <- str_trim(corpus.tmp, side= "both")
Corpus <- lapply(corpus.tmp, function(x){x<- as.String(x)})
Corpus
corpus.tmp
corpus.tmp
Corpus.tagged <- lapply(Corpus, function(x){
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(newtable, list(sent_token_annotator,
word_token_annotator))
y2 <- annotate(newtable, pos_tag_annotator, y1)
y2t <- subset(y2, type == "word")
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
r1 <- sprintf("%t/%s", y2t, tags)
r2 <- paste(r1, collapse = " ")
return(r2)})
corpus.tmp <- lapply(my.text.begin.end, function(x){x <- paste(x, sep = " ")})
corpus.tmp
my.text.begin.end<-paste("<begin> ",my.text," <end>")
corpus.tmp <- lapply(my.text.begin.end, function(x){x <- paste(x, collapse = " ")})
corpus.tmp <- lapply(corpus.tmp, function(x){x<- enc2utf8(x)})
corpus.tmp <- str_trim(corpus.tmp, side= "both")
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(corpus.tmp, list(sent_token_annotator,
word_token_annotator))
text.paste <- paste(my.text.begin.end, collapse = " ")
text.paste
text.punct <- gsub("([[:punct:]])"," \\1 ", text.paste )
text.punct
text.punct <- gsub("[\\.!?:;]"," \\1 ", text.paste )
text.punct
text.punct <- gsub("([\\.!?:;])"," \\1 ", text.paste )
text.punct
my.text
Encoding(my.text) <-"UTF-16"
my.text
Encoding(my.text) <-"UTF-8"
my.text
my.text<-twit.csv$text
iconv(tweet$text, "latin1", "ASCII", sub="")
my.text<-iconv(twit.csv$text, "latin1", "ASCII", sub="")
my.text
my.text.begin.end<-paste("<begin> ",my.text," <end>")
my.text.begin.end
text.paste <- paste(my.text.begin.end, collapse = " ")
text.punct <- gsub("([\\.!?:;])"," \\1 ", text.paste )
text.punct
text.punct <- gsub("\\. "," \\1 ", text.paste )
text.punct
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2 ", text.punct )
text.punct
text.punct <- gsub("\\. "," \\1 ", text.paste )
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2", text.punct )
text.punct
text.punct <- gsub("([a-zA-Z])(_)","\\1 \\2", text.punct )
text.punct
text.punct <- gsub("([a-zA-Z])(:)","\\1 \\2", text.punct )
text.punct
text.punct <- gsub("([a-zA-Z])(\\.)","\\1 \\2", text.paste )
text.punct
text.punct <- gsub("([a-zA-Z])(\\. )","\\1 \\2", text.paste )
text.punct
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2", text.punct ) # keep !!! together
text.punct <- gsub("([a-zA-Z])(_)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(:)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(,)","\\1 \\2", text.punct )
text.punct
text.punct <- gsub("([a-zA-Z])(\\. )","\\1 \\2", text.paste )
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2", text.punct ) # keep !!! together
text.punct <- gsub("([a-zA-Z])(_)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(: )","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(,)","\\1 \\2", text.punct )
text.punct
corpus.tmp <- str_trim(text.punct, side= "both")
corpus.tmp
Corpus <- str_trim(text.punct, side= "both")
corpus <- str_trim(text.punct, side= "both")
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
corpus
unlist(corpus)
as.character(corpus)
mycorpus <-corpus[1]
mycorpus
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(mycorpus, list(sent_token_annotator,
word_token_annotator))
class(mycorpus)
corpus.trim <- str_trim(text.punct, side= "both")
corpus <- as.String(corpus.trim)  # it looks like it was still a list
class(mycorpus)
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
corpus
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
?annotate
?as.String
is.string(corpus)
is.String(corpus)
sent_token_annotator
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
annotate(corpus, sent_token_annotator)
library(openNLPmodels.en)
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
install.packages("openNLPdata")
library("openNLPdata", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("openNLPmodels.es", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library("openNLP", lib.loc="/Library/Frameworks/R.framework/Versions/3.3/Resources/library")
library(openNLPdata)
library(openNLPmodels.en)
install.packages("openNLPmodels.en", repos = "http://datacube.wu.ac.at/", type = "source")
library(openNLPmodels.en)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
library(NLP)
library(tm)
library(stringr)
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
y2 <- annotate(corpus, pos_tag_annotator, y1)
y2t <- subset(y2, type == "word")
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
r1 <- sprintf("%t/%s", y2t, tags)
r1 <- sprintf("%s\t%s", y2t, tags) # format the  DT
r1
r1 <- sprintf("%s\t%s", y2t, tags) # format the  DT
y2w <- corpus[subset(y1, type == "word")]
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
r1 <- sprintf("%s\t%s", y2w, tags) # format the  DT
r1
library(openNLPmodels.es)
library(openNLPdata)
my.text.begin.end<-paste("BEGIN ",my.text," END")
text.paste <- paste(my.text.begin.end, collapse = " ")
text.punct <- gsub("([a-zA-Z])(\\. )","\\1 \\2", text.paste )
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2", text.punct ) # keep !!! together
text.punct <- gsub("([a-zA-Z])(_)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(: )","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(,)","\\1 \\2", text.punct )
corpus.trim <- str_trim(text.punct, side= "both")
corpus <- as.String(corpus.trim)  # it looks like it was still a list
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
y2 <- annotate(corpus, pos_tag_annotator, y1)
# y2t <- subset(y2, type == "word")
y2w <- corpus[subset(y1, type == "word")]
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
r1 <- sprintf("%s\t%s", y2w, tags) # format the  DT
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
y2w <- corpus[subset(y1, type == "word")]
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
r1 <- sprintf("%s\t%s", y2w, tags) # format the  DT
text.punct
corpus
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
y2 <- annotate(corpus, pos_tag_annotator, y1)
y2t <- subset(y2, type == "word")
y2w <- corpus[subset(y1, type == "word")]
y2w
text.punct <- gsub("!+","!",text.paste)
text.punct
text.punct <- gsub("_+","_",text.paste)
text.punct <- gsub("!+","!",text.paste)
text.punct <- gsub("_+","_",text.punct)
text.punct <- gsub("\\.+","\\.",text.punct)
text.punct <- gsub("([a-zA-Z])(\\. )","\\1 \\2", text.paste )
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2", text.punct ) # keep !!! together
text.punct <- gsub("([a-zA-Z])(_)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(: )","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(,)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(\\. )","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(_)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(: )","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(,)","\\1 \\2", text.punct )
text.punct
text.punct <- gsub("!+","!",text.paste)
text.punct <- gsub("_+","_",text.punct)
text.punct <- gsub("\\.+","\\.",text.punct)
text.punct <- gsub("([a-zA-Z])(\\. )","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(!)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(_)","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(: )","\\1 \\2", text.punct )
text.punct <- gsub("([a-zA-Z])(,)","\\1 \\2", text.punct )
text.punct
corpus.trim <- str_trim(text.punct, side= "both")
corpus <- as.String(corpus.trim)  # it looks like it was still a list
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator()
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
y2 <- annotate(corpus, pos_tag_annotator, y1)
y2t <- subset(y2, type == "word")
y2w <- corpus[subset(y1, type == "word")]
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
r1 <- sprintf("%s\t%s", y2w, tags) # format the  DT
r1
pos_tag_annotator <- Maxent_POS_Tag_Annotator(language = "es")
library(openNLPmodels.es)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(language = "es")
pos_tag_annotator <- Maxent_POS_Tag_Annotator(language = "spa")
pos_tag_annotator <- Maxent_POS_Tag_Annotator(probs = TRUE,language = "es")
install.packages("~/Documents/CL/Classes/Corpus_Linguistics/Course2016/Projects/openNLPmodels.es_0.0-4.tar.gz", repos = NULL, type = "source")
library(openNLPmodels.es)
sent_token_annotator <- Maxent_Sent_Token_Annotator()
word_token_annotator <- Maxent_Word_Token_Annotator()
pos_tag_annotator <- Maxent_POS_Tag_Annotator(probs = TRUE,language = "es")
sent_token_annotator <- Maxent_Sent_Token_Annotator(language = "es",probs = FALSE, model =NULL)
sent_token_annotator <- Maxent_Sent_Token_Annotator(probs = FALSE, model =NULL)
word_token_annotator <- Maxent_Word_Token_Annotator(probs = FALSE, model =NULL)
pos_tag_annotator <- Maxent_POS_Tag_Annotator(probs = FALSE, model =NULL)
y1 <- annotate(corpus, list(sent_token_annotator,
word_token_annotator))
y2 <- annotate(corpus, pos_tag_annotator, y1)
y2t <- subset(y2, type == "word")
y2w <- corpus[subset(y1, type == "word")]
tags <- sapply(y2t$features, '[[', "POS")  # List of pos tags
r1 <- sprintf("%s\t%s", y2w, tags) # format the  DT
r1
my.names <-twit.csv$screenName
my.names
x
r2 <- sprintf("%s\%s", y2w, tags)
r2 <- sprintf("%s/%s", y2w, tags)
setwd("~/Documents/MCS/TextMiningClass/Project/TextMiningBetaJay")
shiny::runApp()
r2
r3 <- paste(r2, sep=" ")
r3
r3 <- paste(r2, collapse=" ")
r3
my.names
n=1
lines<-vector()
line1 <- r1[1]
line1
line1=="BEGIN.*"
grepl("BEGIN.*",line1)
n=1
lines<-vector()
for (i in 1:length(r1)) {
line1 <- r1[1]
if (grepl("BEGIN.*",line1)==TRUE) {
line1<-paste(line1,"\t", my.names[n])
n=n+1
} else{
line1 <-line1}
lines[1]<-line1
}
lines
n=1
lines<-vector()
for (i in 1:length(r1)) {
line1 <- r1[i]
if (grepl("BEGIN.*",line1)==TRUE) {
line1<-paste(line1,"\t", my.names[n])
n=n+1
} else{
line1 <-line1}
lines[i]<-line1
}
lines
write.csv(lines, file = '~/Desktop/twit-pos-name.csv', row.names=F, fileEncoding = "UTF-8")
write.csv(lines, file = '~/Desktop/twit-pos-name.csv', quote=FALSE,row.names=F, fileEncoding = "UTF-8")
write.csv(lines, file = '~/Desktop/twit-pos-name.csv', quote=FALSE,row.names=F, fileEncoding = "UTF-8")
write(lines,file = '~/Desktop/twit-pos-name.txt', quote=FALSE,row.names=F, fileEncoding = "UTF-8")
write(lines,file = '~/Desktop/twit-pos-name.txt', quote=FALSE,fileEncoding = "UTF-8")
write(lines,file = '~/Desktop/twit-pos-name.txt', fileEncoding = "UTF-8")
write(lines,file = '~/Desktop/twit-pos-name.txt')
runApp()
