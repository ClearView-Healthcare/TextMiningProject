x11()
plot(ItaRi.emp.vgc, ItaRi.bin.vgc,legend=c("observed","interpolated"))
shiny::runApp('R/Tweeter/TextMining')
install.packages("quanteda")
install.packages("installr")
install.packages(quanteda, dependencies = TRUE, repos='http://cran.rstudio.com/')
install.packages("quanteda", dependencies = TRUE, repos='http://cran.rstudio.com/')
updateR()
shiny::runApp('R/Tweeter/TextMining')
install.packages("quanteda")
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
load(DATA)
load("DATA")
out <- sentSplit(DATA, "state"))
out <- sentSplit(DATA, "state")
head(out)
sentSplit(DATA, "state", stem.col = TRUE)
out %&% check_text()
sentSplit(DATA, "state", text.place = "left")
plot(out)
plot(out, grouping.var = "person")
x <- with(DATA, word_length(state, person))
plot(x)
input.txt <- "/Users/olgascrivner/R/Tweeter/Jefferson/DavisScrivnerHadoop/dante"
files.txt <- list.files(input.txt, pattern="txt",full.names = TRUE)
text.scan <- scan(files.txt[1], what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
corpus <- Corpus(VectorSource(data))
corpus
data.paste <- paste(data, collapse = " ")
novel.vector <-vector() # for lda
text.punct <- removeNumbers(data.paste)
novel.vector[i] <- text.punct
novel.vector[1] <- text.punct
corpus <- Corpus(VectorSource(novel.vector))
corpus
tdm <- TermDocumentMatrix(corpus)
m <- as.matrix(tdm)
m
tdm
df <- as.data.frame(tdm)
df <- as.data.frame(novel.vector)
df
m[[1]]
m[1]
m
tab <- as.data.frame(m)
tab
dtm <-DocumentTermMatrix(corpus)
Terms(dtm)
nTerms(dtm)
nDocs(dtm)
df <- as.data.frame(cbind(Terms(dtm),Docs(dtm)))
df
x <- with(df word_length(V1,V2))
x <- with(df, word_length(V1,V2))
plot(x)
x
x2 <- word_length(df[["V1"]])
plot(x2)
x2
proportions(x2)
p <- proportions(x2)
plot(p)
class(p)
p[1]
p[2]
p[3]
length(p)
p[[1]]
p[[3:length(p)]]
p[3:length(p)]
p[3]
p[1,1]
p[3,1]
p[2,1]
p[1,3]
tab <- as.data.frame(p)
tab
tab[1]
m <- as.matrix(tab)
m
df <- as.data.frame(rbind(Terms(dtm),Docs(dtm)))
df
df[1,]
df[2,]
out <- readCorpus(dtm, type="dtm")
documents <- out$documents
vocab <- out$vocab
table(vocab,documents)
documents
vocab
as.matrix(vocab,Docs(dtm))
as.matrix(Terms(dtm),Docs(dtm))
m <- as.matrix(Terms(dtm),Docs(dtm))
word_length(m)
wl <- word_length(m)
class(wl)
length(wl)
wl[3]
wl[1]
wl[2]
wl[3]
wl$percent
scores(wl)
counts(wl)
wl$prop[3]
wl$prop[3:16]
plot(wl$prop[3:16])
plot(wl$counts[3:16])
plot(wl$count[3:16])
wl$count[3:16]
plot(wl$count[3:4])
wl$percent[10:16]
wl$percent
wl$prop[10:16]
plot(wl$prop[10:16])
plot(proportions(x[10:16]))
proportions(x[10:16])
proportions(x)
plot(proportions(x))
wl
colnames(wl)
rownames(wl)
colnames(m)
rownames(m)
m
wl <- word_length(m[,1])
wl
as.data.frame(wl)
as.matrix(wl)
wl
as.table(wl)
wl[3:16]
as.table(wl$rnp)
wl$rnp
class(wl$rnp)
wl-df <- wl$rnp
wldf <- wl$rnp
wldf
colnames(wldf)
rownames(wldf)
colnames(wldf[3:16])
wldf[3:16]
cbind(colnames(wldf[3:16],wldf[3:16]))
cbind(colnames(wldf[3:16]),wldf[3:16])
t(wldf[3:16])
tab <- t(wldf[3:16])
plot(tab)
tab[,1]
wl$rnp
wldf <- wl$prop
tab <- t(wldf[3:16])
plot(tab)
ggplot(tab)+ geom_bar()
ggplot(tab[,1])+ geom_bar()
ggplot(wldf)+ geom_bar()
ggplot(wldf[3:16])+ geom_bar()
ggplot(x=rownames(tab),y=tab[,1])+ geom_bar()
rownames(tab)
ggplot(x=as.numeric(rownames(tab)),y=tab[,1])+ geom_bar()
tab[,1]
ggplot(y=tab[,1])+ geom_bar()
ggplot(y=tab[,1])
plot(tab)
tab
class(tab)
as.table(tab)
dt <- as.table(tab)
plot(dt)
ggplot(dt)
cbind(dt,rownames(tab))
as.numeric(cbind(dt,rownames(tab)))
cbind(as.numeric(dt),rownames(tab))
cbind(as.numeric(dt),as.numeric(rownames(tab)))
newtab <- cbind(as.numeric(dt),as.numeric(rownames(tab)))
newtab <- cbind(as.numeric(dt),as.numeric(rownames(tab)),colnames=c("proportion","word length"))
newtab
newtab <- as.matrix(cbind(as.numeric(dt),as.numeric(rownames(tab))),colnames=c("proportion","word length"))
newtab
newtab <- as.data.frame(cbind(as.numeric(dt),as.numeric(rownames(tab))),colnames=c("proportion","word length"))
newtab
newtab <- as.data.frame(cbind(as.numeric(dt),as.numeric(rownames(tab))))
newtab
colnames(newtab) <- c("proportion","word length")
newtab
ggplot(newtab)
+ geom_bar()
geom_line()
ggplot(newtab) + geom_line()
colnames(newtab) <- c("proportion","word_length")
ggplot(newtab, aes(x=word_length,y=proportion)) + geom_bar()
ggplot(newtab, aes(x=word_length,y=proportion)) + geom_line()
length(wldf$prop)
length(wldf)
runApp('R/Tweeter/TextMining')
shiny::runApp('R/Tweeter/TextMining')
input.txt <- "/Users/olgascrivner/R/Tweeter/Jefferson/DavisScrivnerHadoop/Shakespeare"
files.txt <- list.files(input.txt, pattern="txt",full.names = TRUE)
text.scan <- scan(files.txt[1], what="character", sep="\n",blank.lines.skip = FALSE)
input.txt <- "/Users/olgascrivner/R/Tweeter/Jefferson/DavisScrivnerHadoop/dante"
files.txt <- list.files(input.txt, pattern="txt",full.names = TRUE)
text.scan <- scan(files.txt[1], what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
vec <- as.data.frame(data)
colnames(vec) <- "Sentence"
sent <- sentSplit(vec,"Sentence")
sent
vec
data <- which(data!="")
data
data=enc2utf8(text.scan)
data <- tolower(data)
data.paste <- paste(data, collapse = " ")
vec <- as.data.frame(data.paste)
vec
colnames(vec) <- "Sentence"
sent <- sentSplit(vec,"Sentence")
sent
datanum <- which(data!="")
data.paste <- data[datanum]
data.paste
data2 <- paste(data.paste, collapse=" ")
data2
vec <- as.data.frame(data2)
vec
colnames(vec) <- "Sentence"
vec
sent <- sentSplit(vec,"Sentence")
sent
lapply(sent, transform, factor=length(sent$Sentence))
sent$split <- strsplit(sent$Sentence," ")
sent
sent$count<-length(sent$split)
sent$count
sent$count<-sum(sent$split != "")
sent
word.count(sent,Sentence)
word_count(sent,Sentence)
word_count(vec,Sentence)
word_count(sent,"Sentence")
class(sent)
word_count(sent)
sent[,2]
word_count(sent[,2])
sent$count<-word_count(sent[,2])
sent
newtab <- sent[1,3,4]
newtab
count <-sent$count
sent <-sent$split
newtab <- as.data.frame(cbind(sent,count))
newtab
sentence <- sent$Sentence
newtab <- as.data.frame(cbind(sentence,count))
newtab
colnames(sent)
colnames(vec)
newtab <- as.data.frame(cbind(vec,count))
newtab
sent
vec
sent[,2]
sent$split
colnames(vec)
sent$split <- strsplit(sent$Sentence," ")
vec <- as.data.frame(data2)
colnames(vec) <- "Sentence"
sent <- sentSplit(vec,"Sentence")
sent
sent[,2]
sent$Sentence
class(sent)
sent$split <- strsplit(sent$Sentence," ")
sent
sent$count<-word_count(sent[,2])
counts <-sent$count
splits <-sent$split
sen <- sent$Sentence
newtab <- as.data.frame(cbind(sen,count))
newtab
runApp('R/Tweeter/TextMining')
newtab
table(counts)
t <- as.data.frame(table(counts))
t
runApp('R/Tweeter/TextMining')
newtab <- as.data.frame(table(counts))
newtab
colnames(newtab) <- c("sentence_length","frequency")
newtab
ggplot(newtab, aes(x=sentence_length,y=frequency)) + geom_line()
ggplot(newtab, aes(x=word_length,y=proportion)) + geom_bar()
ggplot(newtab, aes(x=sentence_length,y=frequency)) + geom_bar()
ggplot(newtab, aes(x=sentence_length,y=frequency)) + geom_line()
plot(newtab)
runApp('R/Tweeter/TextMining')
table(counts)
t<- table(counts)
plot(t)
prop.table(t)
tprop <- prop.table(t)
plot(tprop)
qplot(tprop)
t[,1]
t[1,]
tprop <- t(prop.table(t))
tprop
t[1]
tprop[1]
tprop[1:10]
newtab <- as.data.frame(tprop)
newtab
length(tprop)
tprop[1:22]
newtab <- as.data.frame(cbind(tprop,t[1:length(t)]))
length(t)
length(tprop)
newtab <- as.data.frame(table(counts))
newtab
colnames(newtab) <- c("sentence","frequency")
newtab
ggplot(newtab, aes(x=sentence,y=frequency)) + geom_line()
ggplot(newtab, aes(x=as.factor(sentence),y=frequency)) + geom_line()
ggplot(newtab, aes(x=as.factor(sentence),y=frequency)) + geom_point()
runApp('R/Tweeter/TextMining')
t<-as.table(counts)
t
t<-table(counts)
t
hist(t)
counts <- as.factor(counts)
counts
t<-table(counts)
hist(t)
t<-table(counts)
t
newtab <- as.data.frame(table(counts))
newtab
newtab[,1] <- as.factor(newtab[,1])
newtab
plot(newtab)
hist(newtab)
runApp('R/Tweeter/TextMining')
newtab <- as.data.frame(counts)
newtab
hist(newtab)
counts <- as.numeric(counts)
hist(newtab)
plot(newtab)
runApp('R/Tweeter/TextMining')
hist(newtab)
counts <- as.numeric(counts)
t<-table(counts)
hist(t)
t
counts <-sent$count
counts
t<- table(counts)
t
newtab <- as.data.frame(counts)
newtab
hist(newtab)
plot(newtab)
counts <- as.factor(counts)
newtab <- as.data.frame(counts)
newtab
plot(newtab)
plot(newtab, xlim=c(0,1,length(t)))
length(t)
plot(newtab, xlim=c(0:22))
plot(newtab, xlim=c(0,22))
plot(newtab, xlim=c(0,10))
plot(newtab, xlim=c(0,1))
plot(newtab, xlim=c(0,1,10))
?xlim
plot(newtab, xlim=c(0,5)
)
plot(newtab, xlim=c(0,20))
plot(newtab, xlim=c(0,50))
plot(newtab, xlim=c(0,40))
runApp('R/Tweeter/TextMining')
shiny::runApp('R/shinyProjects/Training/Pages')
runApp('R/shinyProjects/Training/Pages')
shiny::runApp('R/shinyProjects/Training/Pages')
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
input.txt <- "/Users/olgascrivner/R/Tweeter/Jefferson/DavisScrivnerHadoop/dante"
files.txt <- list.files(input.txt, pattern="txt",full.names = TRUE)
text.scan <- scan(files.txt[1], what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
data <- tolower(data)
data.paste <- paste(data, collapse = " ")
vec <- as.data.frame(data.paste)
vec
colnames(vec) <- "Sentence"
sent <- sentSplit(vec,"Sentence")
sent
data.paste <- gsub("\\s\\s+","",data.paste)
data.paste
data=enc2utf8(text.scan)
data <- tolower(data)
data.paste <- paste(data, collapse = " ")
data.paste <- gsub("\\s+"," ",data.paste)
data.past
data.paste
vec <- as.data.frame(data.paste)
colnames(vec) <- "Sentence"
sent <- sentSplit(vec,"Sentence")
sent
sent$split <- strsplit(sent$Sentence," ")
sent
data.punct <- gsub("([.?!;])|[[:punct:]]","\\1",data.paste)
data.punct
sent$split <- strsplit(sent$Sentence," ", endmarks = c("?","!",".",";",":"))
sent
data.punct <- gsub("([.?!;:])|[[:punct:]]","\\1",data.paste)
vec <- as.data.frame(data.punct)
colnames(vec) <- "Sentence"
sent <- sentSplit(vec,"Sentence",endmarks = c("?","!",".",";",":"))
sent
data.punct <- gsub("\\s+"," ",data.punct)
vec <- as.data.frame(data.punct)
colnames(vec) <- "Sentence"
sent <- sentSplit(vec,"Sentence",endmarks = c("?","!",".",";",":"))
sent
vec
sent$split <- strsplit(sent$Sentence," ")
sent$count<-word_count(sent[,2])
counts <-sent$count
splits <-sent$split
sen <- sent$Sentence
counts
t<- table(counts)
t
counts <- as.factor(counts)
t<- table(counts)
t
tp <- prop.table(counts)
counts <-sent$count
tp <- prop.table(counts)
tp
tp <- prop.table(counts,2)
tp <- prop.table(table(counts),2)
tp <- prop.table(as.table(counts),2)
t<- table(counts)
t
as.data.frame(t)
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
hist(counts)
x11()
hist(counts)
?hist
hist(counts,density=TRUE,main="Histogram of Sentence Length", col="black")
hist(counts,density=TRUE,main="Histogram of Sentence Length", col="gray")
hist(counts,density=TRUE,main="Histogram of Sentence Length", col="black",freq=FALSE)
hist(counts,density=TRUE,main="Histogram of Sentence Length", col="black",freq=FALSE)
hist(counts,main="Histogram of Sentence Length", col="black",freq=FALSE)
vec <- as.data.frame(data.punct)
hist(counts,main="Histogram of Sentence Length", col="black",freq=TRUE)
hist(counts,main="Histogram of Sentence Length", col="grey",freq=TRUE)
runApp('R/Tweeter/TextMining')
?lexicalize
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
shiny::runApp('R/Tweeter/TextMining')
seq(0,10, length.out=40)
shiny::runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
input.txt <- "/Users/olgascrivner/R/Tweeter/transcripts"
files.txt <- list.files(input.txt, pattern="txt",full.names = TRUE)
text.scan <- scan(files.txt[2], what="character", sep="\n",blank.lines.skip = FALSE)
data=enc2utf8(text.scan)
data.paste <- paste(data, collapse = " ")
name.speaker <- "E:"
name.interviewer <- "I:"
new.line <- gsub(name.speaker, "NEWLINE SPEAKER",data.paste)
new.line <- gsub(name.interviewer, "NEWLINE INTERVIEWER",new.line)
split.sub <- strsplit(new.line,"NEWLINE")
split.sub
text.punct <-grep("SPEAKER", split.sub, value=TRUE)
text.punct
split.sub <- unlist(strsplit(new.line,"NEWLINE"))
split.sub[[1]]
split.sub[[2]]
speaker <-grep("SPEAKER", split.sub)
speaker[2]
text.punct <- split.sub[speaker]
text.punct
runApp('R/Tweeter/TextMining')
htm <- c("\\","/","//")
txt <- gsub(htm,"",text.punct)
htm <- c("/","//")
txt <- gsub(htm,"",text.punct)
txt
runApp('R/Tweeter/TextMining')
shiny::runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
?stopwords
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
runApp('R/Tweeter/TextMining')
library("shinyapps", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
setwd("/Users/olgascrivner/R/Tweeter/TextMining")
deployApp(appName="TextMining")
source("http://bioconductor.org/biocLite.R")
biocLite()
deployApp(appName="TextMining")
